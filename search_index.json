[["index.html", "PhD Course 2021: Linux containers in scientific environments Part 1 Welcome and about the course", " PhD Course 2021: Linux containers in scientific environments Luca Cozzuto1, Toni Hermoso2, Julia Ponomarenko3 Part 1 Welcome and about the course This slow-paced hands-on course is designed for absolute beginners who want to start using containers to achieve reproducibility of data analysis. Linux containers allow the storage of code and applications in a host-independent lightweight environment. They became a fast and popular way to share and deploy applications in different environments. CRG, luca.cozzuto@crg.eu↩︎ CRG, toni.hermoso@crg.eu↩︎ CRG, julia.ponomarenko@crg.eu↩︎ "],["meet-the-organizers-and-instructors.html", "1.1 Meet the organizers and instructors", " 1.1 Meet the organizers and instructors Contacts below: CRG Training unit: Damjana Anna Main speakers: Toni Instructors/helper: Luca Julia "],["about-the-course.html", "1.2 About the course", " 1.2 About the course 1.2.1 Outline 1.2.2 Learning objectives About Linux containers: Locate and fetch Docker/Singularity images from dedicated repositories. Execute/Run a Docker/Singularity container from the command line. Build a Docker container from an existing recipe. Design/Write a Docker recipe. Convert Docker to a Singularity image. 1.2.3 Prerequisite / technical requirements 1.2.3.1 Prerequisite Being comfortable working with the CLI (command-line interface) in a Linux-based environment. Applicants are not expected to have used Linux containers or Nextflow before. 1.2.3.2 Technical requirements You should be able to work in a terminal on your machine to access the cloud via ssh: on Linux and Mac operating systems it is there by default. on Windows, you could install a Windows terminal or an application like putty. If you are not familiar with the ssh protocol, you can learn about it here. You should also be able to use a command-line/screen-oriented text editor (such as nano or vi/vim) or to be able to use an editor able to connect remotely like Visual studio, emacs or bbedit. If you are not sure what text editor to use, we recommend that you learn the basics of “nano”. During the course, we will use the Slack platform for discussion and troubleshooting. You can either install the Slack app before the course, or you will be able to access Slack from a web browser. We also encourage you to create a Github account and an associated Docker hub account. 1.2.3.3 Provided infrastructure … 1.2.4 Dates, time, location Location: virtual, via Zoom. You can find all information in the dedicated course page of the Moodle platform. "],["program.html", "1.3 Program", " 1.3 Program Containers: introduction and history. Docker hub: find existing containers. Fetch (and build) an image from public registries (Docker Hub, Quay.io, etc.) Discover relevant base images Run Docker container based on an existing image (also run it interactively). Build an image from an existing recipe: Sections and keywords. Build options (e.g. cache or build variables). Write a Docker recipe and build an image from it. Docker: Work with volumes and ports. Upload image to registries. Singularity: Singularity versus Docker. Differences, pros and cons for each system. Fetch (and build) a Singularity images. Build from existing public registries. Build from local Docker instances. Run a Singularity container (interactively). Understanding and working with volumes. Singularity build recipes. Advanced features: services, etc. "],["introduction-to-containers.html", "Part 2 Introduction to containers ", " Part 2 Introduction to containers "],["what-are-containers.html", "2.1 What are containers ?", " 2.1 What are containers ? A Container can be seen as a minimal virtual environment that can be used in any Linux-compatible machine (and beyond). Using containers is time- and resource-saving as they allow: Controlling for software installation and dependencies. Reproducibility of the analysis. Containers allow us to use exactly the same versions of the tools. "],["virtual-machines-or-containers.html", "2.2 Virtual machines or containers ?", " 2.2 Virtual machines or containers ? Virtualisation Containerisation (aka lightweight virtualisation) Abstraction of physical hardware Abstraction of application layer Depends on hypervisor (software) Depends on host kernel (OS) Do not confuse with hardware emulator Application and dependencies bundled all together Enable virtual machines:Every virtual machine with an OS (Operating System) 2.2.1 Virtualisation Abstraction of physical hardware Depends on hypervisor (software) Do not confuse with hardware emulator Enable virtual machines: Every virtual machine with an OS (Operating System) 2.2.2 Containerisation (aka lightweight virtualisation) Abstraction of application layer Depends on host kernel (OS) Application and dependencies bundled all together 2.2.3 Virtual machines vs containers Source Pros and cons Virtualisation Containerisation PROS Very similar to a full OS. With current solutions, high OS diversity. No need of full OS installation (less space). Faster than virtual machines. Easier automation. Current solutions allow easier distribution of recipes. Better portability. CONS Need more space and resources. Slower than containers. Not that good automation. Some cases might not be exactly the same as a full OS. Still less OS diversity, even with current solutions. "],["history-of-containers.html", "2.3 History of containers", " 2.3 History of containers 2.3.1 chroot chroot jail (BSD jail): first concept in 1979 Notable use in SSH and FTP servers Honeypot, recovery of systems, etc. Source: https://sysopsio.wordpress.com/2016/09/09/jails-in-linux/ 2.3.2 Additions in Linux kernel First version: 2008 cgroups (control groups), before “process containers” isolate resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes Linux namespaces one set of kernel resources restrict to one set of processes Source: https://sysopsio.wordpress.com/2016/09/09/jails-in-linux/ "],["docker.html", "Part 3 Docker ", " Part 3 Docker "],["introduction-to-docker.html", "3.1 Introduction to Docker", " 3.1 Introduction to Docker 3.1.1 What is Docker? Platform for developing, shipping and running applications. Infrastructure as application / code. First version: 2013. Company: originally dotCloud (2010), later named Docker. Established Open Container Initiative. As a software: Docker Community Edition. Docker Enterprise Edition. There is an increasing number of alternative container technologies and providers. Many of them are actually based on software components originally from the Docker stack and they normally try to address some specific use cases or weakpoints. As a example, Singularity, that we introduce later in this couse, is focused in HPC environments. Another case, Podman, keeps a high functional compatibility with Docker but with a different focus on technology and permissions. 3.1.2 Docker components Read-only templates. Containers are run from them. Images are not run. Images have several layers. 3.1.3 Images versus containers Image: A set of layers, read-only templates, inert. An instance of an image is called a container. When you start an image, you have a running container of this image. You can have many running containers of the same image. “The image is the recipe, the container is the cake; you can make as many cakes as you like with a given recipe.” https://stackoverflow.com/questions/23735149/what-is-the-difference-between-a-docker-image-and-a-container 3.1.4 Docker vocabulary Get all the docker commands: docker Get help on a particular command: docker run --help "],["using-existing-images.html", "3.2 Using existing images", " 3.2 Using existing images 3.2.1 Getting started Run the following in the terminal: docker images The docker images command lists the Docker images that you have on your computer. Now run the following: docker pull hello-world Run docker images again: now you see the “hello-world” image listed! docker pull imports by default an image from Docker Hub. We will see in more details the docker images and docker run commands, but let’s first explore the Docker images repositories. 3.2.2 Explore Docker Hub Images can be stored locally or shared in a registry. Docker hub is the main public registry for Docker images. Let’s search the keyword ubuntu: You can also search existing Docker images with the docker search command. Example: let’s look for images that have the keyword blast in their name or description. docker search blast Too many results? You can apply some filters: Minimum number of stars: docker search blast --filter stars=5 The image is an official build: docker search blast --filter is-official=true The image is an automated build: docker search blast --filter is-automated=true # Apply one filter docker search blast --filter stars=2 # Apply more than one filter docker search blast --filter is-automated=true --filter stars=2 https://vsupalov.com/docker-latest-tag/ to read more about the latest tag HANDS-ON Use docker search to find a Docker image for the keyword ubuntu. Using the filters, answer the following questions: How many images are official builds? How many images have 3 or more stars? How many images are official builds AND have 3 or more stars? What is the NAME of the image with the highest number of stars? Answer # Official builds docker search ubuntu --filter is-official=true # 3 or more stars docker search ubuntu --filter stars=3 # Both filters docker search ubuntu --filter is-official=true --filter stars=3 3.2.3 docker pull: import an image Say we are now interested in the ubuntu image from Docker Hub. We can retrieve it with docker pull. By default, we get the latest image / latest release. docker pull ubuntu You can choose the version of Ubuntu you are fetching; for that, check different tags on the website (latest is also a tag): Note: docker search doesn’t allow to search for tags. Let’s get the Ubuntu image with tag 18.04 (version 18.04 of Ubuntu = bionic): docker pull ubuntu:18.04 Where is the image now? As we have seen before, you can run docker images in the terminal to see a list of the most recently created images. The command docker images gives you information about: Repository. Tag. Unique image ID (Digest). Creation date. Image size. Notice that for some images, such as Ubuntu, the same version tag hosts several Unique Image IDs (Digest). Each ID corresponds to a build for a specific Operating System or Architecture. Docker clients automatically retrieves the suitable one when using docker pull. If the OS/Architecture from the client is not present, an error is raised. IMPORTANT: Digest ID present in Docker hub is NOT the same (or a shortened version) of the Image ID you can see when doing docker images. Details: https://stackoverflow.com/questions/56364643/whats-the-difference-between-a-docker-images-image-id-and-its-digest It is actually possible, and it can be advisable for reproducibility reasons, to pull an image with a specific digest ID. Example: docker pull ubuntu@sha256:86ac87f73641c920fb42cc9612d4fb57b5626b56ea2a19b894d0673fd5b4f2e9 HANDS-ON Run command docker images. How many images do you get? Pull the version 2.2.31 of the biocontainers/blast image What is the size of the blast image you just pulled? How many images do you get if you run docker images --all? What are these images? Check documentation for help. Answer # Pull the blast image docker pull biocontainers/blast:2.2.31 # Run `docker images --all` docker images --all # intermediate images. 3.2.4 docker run: run image, i.e. start a container Now we want to use what is inside the image. Command docker run creates a fresh container (active instance of the image) from a Docker (static) image, and runs it. The format is: docker run image:tag command (command being a command called inside the image) We can start a container from the ubuntu tag 18.04 image, executing the command ls (stored in /bin in the container). docker run ubuntu:18.04 /bin/ls Now execute ls in your current working directory: is the result the same? You can execute any program/command that is stored inside the image: docker run ubuntu:18.04 /bin/whoami docker run ubuntu:18.04 cat /etc/issue You can either execute programs in the image from the command line (see above) or execute a container interactively; that is, “enter” the container, using command docker run -it. docker run -it ubuntu:18.04 /bin/bash If you want to leave and stop the container, type exit and ENTER. HANDS-ON Run the hello-world image: What is happening? Now run the blast image we previously pulled: Is something happening? Start again a container from the same blast image (not interactively), and run blastp. What happens? Start a container interactively from the same blast image: What is the default working directory? What is inside this directory? Where is the blastp program located in the image? Exit the container. Answer # Run the hello-world image docker run hello-world # Run the blast image docker run biocontainers/blast:2.2.31 # Start again a container from the same blast image, and run the path to the blastp command: docker run biocontainers/blast:2.2.31 blastp # Start a container interactively from the same blast image: docker run -ti biocontainers/blast:2.2.31 # What is the default working directory? pwd; ls # Where is the `blastp` program located in the image? which blastp # Exit the container. exit Note about Docker inside Docker: “Although running Docker inside Docker is generally not recommended, there are some legitimate use cases, such as development of Docker itself.” https://hub.docker.com/_/docker You can run the container as daemon (in background), instead of the default foreground running, with the --detach parameter: docker run --detach ubuntu:18.04 tail -f /dev/null Run container as daemon (in background) with a given name: docker run --detach --name myubuntu ubuntu:18.04 tail -f /dev/null 3.2.5 docker ps: check containers status List running containers: docker ps List all containers (whether they are running or not): docker ps -a Each container has a unique ID. 3.2.6 docker exec: execute process in a running container Difference between docker run and docker exec: docker run creates a temporary container, runs the command and stops the container. docker exec needs an already running container to query the command (that is, a detached container). docker exec myubuntu uname -a Interactively docker exec -it myubuntu /bin/bash 3.2.7 docker stop, start, restart: actions on container Stop a running container with docker stop. # check the list of running containers docker ps # stop the myubuntu container docker stop myubuntu # check the list of all containers docker ps -a Start a stopped container (does NOT create a new one): docker start myubuntu docker ps -a Restart a running container: docker restart myubuntu docker ps -a Run with restart is enabled (by default, when exits, Docker does not automatically restart the container). In the example below, we start a detached container named “myubuntu2” with the unless-stopped restart policy: restart the container unless it is explicitly stopped or Docker itself is stopped or restarted. docker run --restart=unless-stopped --detach --name myubuntu2 ubuntu:18.04 tail -f /dev/null Restart policies: no (default), always, on-failure, unless-stopped Update restart policy: docker update --restart unless-stopped myubuntu HANDS-ON Start a container from the “hello-world” image in the background. Give it a name. Is your container running? Can you explain why (or why not)? Start another detached container from the same image (with a new name), with the always restart policy. Is the container running? Answer # start a &quot;detached&quot; container (in the background) docker run --detach --name helloworld1 hello-world # start a &quot;detached&quot; container with the &quot;--restart=always&quot; option docker run --detach --restart=always --name helloworld2 hello-world 3.2.8 docker rm, docker rmi: clean up! docker rm is used to remove a container (set -f is the container is running, to force the removal): docker rm myubuntu docker rm -f myubuntu docker rmi is used to remove an image: docker rmi ubuntu:18.04 HANDS-ON Remove any container (whether it is running or not). Remove the “hello-world” image. Answer # check all containers docker ps -a # remove by their ID: docker rm -f CONTAINER1_ID CONTAINER2_ID ... # remove the &quot;hello-world&quot; image docker rmi hello-world 3.2.8.1 Major clean Check used space: docker system df Remove unused containers (and others) - DO WITH CARE docker system prune Remove ALL non-running containers, images, etc. - DO WITH MUCH MORE CARE!!! docker system prune -a Reference "],["exercise-1---docker-as-a-user.html", "3.3 Exercise 1 - Docker as a user", " 3.3 Exercise 1 - Docker as a user In breakout rooms, do the following exercise: alpine image. Search and pull the alpine image (tag 3.12) - it is an official build. Can you run a container from this image and make it print a “hello world” message? Now run a container interactively from the same image. Run whoami in the container. Exit the container and run whoami on the host machine: do you get the same output? Restart the container you just exited: Is it now running? Make the container execute the command ls. Stop the container. Remove the alpine image and all its containers (running or stopped). Answer # Search and pull the alpine image (tag 3.12) - it is an official build. docker search alpine --filter is-official=true docker pull alpine:3.12 # Can you run a container from this image and make it print a “hello world” message? docker run alpine:3.12 echo &quot;hello world&quot; # Now run a container **interactively** from the same image. docker run -ti alpine:3.12 # Run `whoami` whoami # Exit the container. exit # Restart the container you just exited: is it now running? docker restart CONTAINER_ID # find it with `docker ps -a` # Make the container execute the command `ls` docker exec CONTAINER_ID ls # Stop the container docker stop CONTAINER_ID # Remove the alpine image and all its containers (running or stopped) docker rmi alpine:3.12 docker rm CONTAINER_ID # check all containers with `docker ps -a` imagemagick Pull the ìmagemagick image that is official and that has the highest number of stars Check the version of the convert command. Start a container interactively. Inside the container: download this png image Convert it to .jpg using the convert command of imagemagick (format; convert image.png image.jpg). Exit the container. Copy the jpg image back from the stopped container! Explore docker cp. Answer # Pull image docker pull acleancoder/imagemagick-full # Check version of `convert` docker run acleancoder/imagemagick-full convert --version # Start interactive container docker run -it acleancoder/imagemagick-full # fetch png image &gt; wget https://pbs.twimg.com/profile_images/1273307847103635465/lfVWBmiW_400x400.png # convert to jpg &gt; convert lfVWBmiW_400x400.png myimage.jpg # exit container # fetch container ID with `ps -a` and use `docker cp` to copy jpg file from the stopped container to the host docker cp *CONTAINER_ID*:/myimage.jpg . "],["linux-packages.html", "3.4 Linux packages", " 3.4 Linux packages In the next topic, we will create Docker images. For this to go smoothly, you should know your base system, and how to interact with it (update, upgrade and install packages for example). In Linux-based environment, software is usually distributed in the form of packages, kept in repositories. Different operating systems have different tools to fetch and install packages: Operating System Format Tool(s) Debian / Ubuntu / Linux Mint / Raspbian .deb apt, apt-cache, apt-get, dpkg CentOS (RedHat) .rpm yum Fedora (RedHat) .rpm dnf FreeBSD .txz make, pkg Alpine apk More on the subject in this post Note that the environment you are working on for this course is CentOS-based During this course, we will build images based on Ubuntu and CentOS. 3.4.1 Update and upgrade packages In Ubuntu: apt-get update &amp;&amp; apt-get upgrade -y In CentOS: yum check-update &amp;&amp; yum update -y 3.4.2 Search and install packages: In Ubuntu: apt search libxml2 apt install -y libxml2-dev In CentOS: yum search libxml2 yum install -y libxml2-devel.x86_64 Note the -y option that we set for updating and for installing. It is an important option in the context of Docker: it means that you answer yes to all questions regarding installation. Conda is an open source package management system and environment management system that runs on Windows, macOS and Linux. Conda allows to quickly find, install, run and update packages and their dependencies. Popular Conda repositories: Anaconda Conda-forge Bioconda If interested in using Conda, you can choose this Docker image: https://hub.docker.com/r/continuumio/miniconda3 "],["docker-recipes.html", "3.5 Docker recipes", " 3.5 Docker recipes The Docker recipe contains a set of instructions and commands that will be used to create/build a Docker image. 3.5.1 Writing recipes and building images All commands should be saved in a text file, named by default Dockerfile. Instruction What it does FROM Sets the base image. RUN Commands to run. MAINTAINER Docker image maintainer. WORKDIR Sets the working directory for the container’s building instructions. SHELL Changes the default shell. ADD / COPY Copy files from source to destination. ARG Sets variables that can be used as the image is built. ENV Sets environment variables. Persists when container is run. ENTRYPOINT Helps configure the container as an executable. CMD Can provide a default executable or arguments to the ENTRYPOINT executable. VOLUME Creates a mount point for an external volume. EXPOSE Exposes network ports on the container. Reference 3.5.1.1 Basic instructions Each row in the recipe corresponds to a layer in the final image. FROM: parent image. Typically, an operating system. This is the base layer. FROM ubuntu:18.04 RUN: the command to execute inside the image filesystem. Think about it this way: every RUN line is essentially what you would run to install programs on a freshly installed Ubuntu OS. RUN apt install wget This is a basic recipe that takes the ubuntu:18.04 image as a base layer, updates and upgrades Linux packages, and installs wget: FROM ubuntu:18.04 RUN apt update &amp;&amp; apt -y upgrade RUN apt install -y wget HANDS-ON Explore this Dockerfile: What is the base layer? What is being installed in the image? How? Answer Base layer: biocontainers/biocontainers:v1.0.0_cv4 Tool blast is installed using conda. 3.5.1.2 Building images from recipes docker build will create/build a Docker image from a Docker recipe. Save the following commands: FROM ubuntu:18.04 RUN apt update &amp;&amp; apt -y upgrade RUN apt install -y wget in a file named Dockerfile docker build implicitely looks for a file named Dockerfile in the current directory: docker build . Same as: docker build --file Dockerfile . Syntax: --file / -f . stands for the context (in this case, current directory) of the build process. This makes sense if copying files from filesystem, for instance. IMPORTANT: Avoid contexts (directories) over-populated with files (even if not actually used in the recipe). In order to avoid that some directories or files are inspected or included (e.g, with COPY command in Dockerfile), you can use .dockerignore file to specify which paths should be avoided. More information at: https://codefresh.io/docker-tutorial/not-ignore-dockerignore-2/ You can define a specific name for the image during the build process. Syntax: -t imagename:tag. If not defined :tag default is latest. docker build -t mytestimage . # same as: docker build -t mytestimage:latest . The last line of installation should be Successfully built …: then you are good to go. Check with docker images that you see the newly built image in the list… Then let’s check the ID of the image and run it! # Get the ID with docker images docker images # Run/start a container using the ID or name docker run f9f41698e2f8 docker run mytestimage 3.5.1.3 More instructions MAINTAINER Who is maintaining the container? MAINTAINER Toni Hermoso Pulido &lt;toni.hermoso@crg.eu&gt; WORKDIR: all subsequent actions will be executed in that working directory WORKDIR ~ SHELL: allows the default shell used for the shell form of commands to be overridden. Use bash as the default shell: SHELL [&quot;/bin/bash&quot;, &quot;-c&quot;] ADD, COPY: add files to the image filesystem Difference between ADD and COPY explained here and here COPY: lets you copy a local file or directory from your host (the machine from which you are building the image) ADD: same, but ADD works also for URLs, and for .tar archives that will be automatically extracted upon being copied. # COPY source destination COPY ~/.bashrc . ENV, ARG: run and build environment variables Difference between ARG and ENV explained here. ARG values: available only while the image is built. ENV values: available for the future running containers. You can use ARG, for example, to specify the version of the base layer you want to use: With a default value ARG UbuntuVersion=18.04 FROM ubuntu:${UbuntuVersion} Without a default value (i.e. the user is expected to provide it upon building) ARG UbuntuVersion FROM ubuntu:${UbuntuVersion} Provide a value for UbuntuVersion as you build the image with --build-arg: docker build --build-arg UbuntuVersion=20.04 . You can also use ARG to build a specific software version in the image. Let’s try it! HANDS-ON Modify the following recipe so you can decide in the command line which version of Python to install. The default version should be 2.7. Pass the argument to install the version 3.8. Build and run first with the default version, and then with the version 3.8. FROM ubuntu:18.04 RUN apt update &amp;&amp; apt upgrade -y RUN apt install -y python2.7 Answer Recipe is saved in Dockerfile_ARG FROM ubuntu:18.04 # Argument PyVersion with default value 2.7 ARG PyVersion=2.7 RUN apt update &amp;&amp; apt upgrade -y RUN apt install -y python${PyVersion} Build the image to get the default version of Python: docker build -t py27 -f Dockerfile_ARG . Build the image to install the version 3.8 of Python instead (via the --build-arg option): docker build --build-arg PyVersion=3.8 -t py38 -f Dockerfile_ARG . Run the image and check if Python of the correct version is installed: docker run py27 python2.7 --help docker run py38 python3.8 --help CMD, ENTRYPOINT: command to execute when generated container starts The ENTRYPOINT specifies a command that will always be executed when the container starts. The CMD specifies arguments that will be fed to the ENTRYPOINT. In the example below, when the container is run without an argument, it will execute echo \"hello world\" (default). If it is run with the argument nice it will execute echo \"nice\". The argument given in CMD will be overridden. FROM ubuntu:18.04 ENTRYPOINT [&quot;/bin/echo&quot;] CMD [&quot;hello world&quot;] Save the above recipe in Dockerfile_hello. Build: docker build -f Dockerfile_hello -t helloimage . Run without an argument: docker run helloimage Now run with an argument: docker run helloimage &quot;nice&quot; Here is a more complex recipe (save it in a text file named Dockerfile_ubuntu): FROM ubuntu:18.04 MAINTAINER Toni Hermoso Pulido &lt;toni.hermoso@crg.eu&gt; SHELL [&quot;/bin/bash&quot;, &quot;-c&quot;] WORKDIR ~ RUN apt-get update &amp;&amp; apt-get -y upgrade RUN apt-get install -y wget ENTRYPOINT [&quot;/usr/bin/wget&quot;] CMD [&quot;https://cdn.wp.nginx.com/wp-content/uploads/2016/07/docker-swarm-hero2.png&quot;] Build the image: docker build -f Dockerfile_ubuntu . Try to run it without and with an argument: # Remember to check the image ID with `docker images` docker run f9f41698e2f8 # with an argument docker run f9f41698e2f8 https://cdn-images-1.medium.com/max/1600/1*_NQN6_YnxS29m8vFzWYlEg.png 3.5.2 docker tag Use docker tag to tag a local image that has, for example, ID “f9f41698e2f8” in the “ubuntu_wget” image name repository with version/tag “1.0”: docker tag f9f41698e2f8 ubuntu_wget:1.0 If the version/tag is not specified, the default is latest. 3.5.3 Build cache Every line in a Dockerfile is an image/layer by itself (you can see all images with docker images --all). Let’s modify the last line in the previous image recipe (Dockerfile_ubuntu) (let’s change the image URL) and rebuild it (even with a different name/tag): FROM ubuntu:18.04 MAINTAINER Toni Hermoso Pulido &lt;toni.hermoso@crg.eu&gt; WORKDIR ~ RUN apt-get update &amp;&amp; apt-get -y upgrade RUN apt-get install -y wget ENTRYPOINT [&quot;/usr/bin/wget&quot;] CMD [&quot;https://cdn-images-1.medium.com/max/1600/1*_NQN6_YnxS29m8vFzWYlEg.png&quot;] docker build -t mytestimage2 -f Dockerfile_ubuntu . It will start from the last line, and will not re-run commands before the modified line as they were already successfully built. It is very convenient for testing and trying new steps, but it may lead to errors when versions are updated (either FROM image or included packages). It is therefore recommended to start from scratch with --no-cache option, when building the image. docker build --no-cache -t mytestimage2 -f Dockerfile_ubuntu . "],["exercise-2---docker-recipes-and-build-images.html", "3.6 Exercise 2 - Docker recipes and build images", " 3.6 Exercise 2 - Docker recipes and build images In breakout rooms, do the following exercise: figlet recipe. Write a docker recipe in a file Dockerfile_figlet that: is based on ubuntu:18.04. echoes “I love containers” by default or any other word/sentence, if given as the argument. Build the image (give the image the name of your choice). Run the image: with no argument. with “Docker course” as an argument. Modify the Dockerfile_figlet: add the MAINTAINER field. update and upgrade Ubuntu packages. install the figlet program. change “echo” to “figlet” in the ENTRYPOINT Build the image. Run the new image, with the default parameters, then with “Docker course” as an argument. Answer Recipe saved in Dockerfile_figlet: FROM ubuntu:18.04 ENTRYPOINT [&quot;echo&quot;] CMD [&quot;I love containers&quot;] Build: docker build --file Dockerfile_figlet -t mytest . Run the image: # with no argument docker run mytest # with argument &quot;Docker course&quot; docker run mytest &quot;Docker course&quot; Recipe: FROM ubuntu:18.04 MAINTAINER Name Surname &lt;name.surname@mail.com&gt; RUN apt-get update &amp;&amp; apt-get upgrade -y RUN apt-get install -y figlet ENTRYPOINT [&quot;figlet&quot;] CMD [&quot;I love containers&quot;] Build the image. docker build --no-cache --file Dockerfile_figlet -t mytestfiglet . Run the new image, with the default parameter, then with “Docker course” as an argument. # run with no argument docker run mytestfiglet # run with argument &quot;Docker course&quot; docker run mytestfiglet &quot;Docker course&quot; Random numbers Copy the following short bash script in a file called random_numbers.bash. #!/usr/bin/bash seq 1 1000 | shuf | head -$1 This script outputs random intergers from 1 to 1000: the number of integers selected is given as the first argument. Write a recipe for an image: Based on centos:7 That will execute this script (with bash) when it is run, giving it 2 as a default argument (i.e. outputs 2 random integers): the default can be changed as the image is run. Build the image. Start a container with the default argument, then try it with another argument. Answer Recipe (in Dockerfile_RN): FROM centos:7 MAINTAINER Name Surname &lt;name.surname@mail.com&gt; # Copy script from host to image COPY random_numbers.bash . # Make script executable RUN chmod +x random_numbers.bash # As the container starts, &quot;random_numbers.bash&quot; is run ENTRYPOINT [&quot;/usr/bin/bash&quot;, &quot;random_numbers.bash&quot;] # default argument (that can be changed on the command line) CMD [&quot;2&quot;] Build and run: docker build -f Dockerfile_RN -t random_numbers . docker run random_numbers docker run random_numbers 10 "],["additional-commands.html", "3.7 Additional commands", " 3.7 Additional commands docker inspect: Get details from containers (both running and stopped). Things such as IPs, volumes, etc. docker logs: Get console messages from running containers. Useful when using with web services. docker commit: Turn a container into an image. It make senses to use when modifying container interactively. However this is bad for reproducibility if no steps are saved. Good for long-term reproducibility and for critical production environments: docker save: Save an image into an image tar archive. docker load: Load an image tar archive to become an image. docker export: Save a container filesystem into a tar archive. docker import: Import a filesystem tar archive into an image (you need to specify a target tag). HANDS-ON Save a previously created image into a tar archive (Look at the command’s options for help). Remove the original image and recover it again. Answer # Let&#39;s save the image in a tar docker save -o random_numbers.tar random_numbers # Remove the original image docker rmi random_numbers # Check existing images now docker images # Recover it docker load &lt; random_numbers.tar # Check now images docker images If you check the tar archives generated thanks to save with the ones using export, you will notice they do not look the same. The former ones ressemble more what you will find in /var/lib/docker (that is where Docker daemon stores its data) and it includes metadata information (so it is not necessary to specify an image tag). On the other hand, tar files generated with export they simply contantain the image filesystem. You lost that way a lot of metadata associated to the original image, such as the tags, but also things such as ENTRYPOINT and CMD instructions. "],["volumes.html", "3.8 Volumes", " 3.8 Volumes Docker containers are fully isolated. It is necessary to mount volumes in order to handle input/output files. Syntax: --volume/-v host:container We can pull the following image to illustrate: docker pull biocontainers/fastqc:v0.11.9_cv7 FastQC is a tool that runs a quality control on .fastq files (a file format that stores nucleotide sequences and their corresponding quality scores). # Create directory and empty file mkdir datatest touch datatest/test # Run a container in the background (--detach) and mount the local volume &quot;datatest&quot; (we map it to directory /scratch inside the container) docker run --detach --volume $(pwd)/datatest:/scratch --name fastqc_container biocontainers/fastqc:v0.11.9_cv7 tail -f /dev/null # Execute the container interactively docker exec -ti fastqc_container /bin/bash &gt; ls -l /scratch &gt; exit Strictly speaking, these type of volumes we showed above are named bind mounts. If interested, there is a more powerful syntax available which allow you to control things such as turning them read-only (with –mount). More details here: https://docs.docker.com/storage/bind-mounts/ More sophisticated kinds of volumes (which are handled by Docker daemon) are explained here: https://docs.docker.com/storage/volumes/ HANDS-ON Copy the 2 fastq files from the Github repository and place them in mounted directory. Run fastqc interactively (inside container): fastqc /scratch/*.gz Run fastqc outside the container Answer # Download test fastq files (manually or using the following commands) and place them in &quot;datatest&quot;: wget -O - https://github.com/biocorecrg/PhD_course_containers_2021/blob/main/testdata/B7_H3K4me1_s_chr19.fastq.gz?raw=true &gt; datatest/B7_H3K4me1_s_chr19.fastq.gz wget -O - https://github.com/biocorecrg/PhD_course_containers_2021/blob/main/testdata/B7_input_s_chr19.fastq.gz?raw=true &gt; datatest/B7_input_s_chr19.fastq.gz # Mount volumes and start a detached container docker run --detach -ti --volume $(pwd)/datatest:/scratch --name fastqc_container_test biocontainers/fastqc:v0.11.9_cv7 # Execute container interactively and run fastqc docker exec -ti fastqc_container_test /bin/bash &gt; fastqc /scratch/*.gz # Run fastqc outside the container # One by one docker exec fastqc_container_test fastqc /scratch/B7_H3K4me1_s_chr19.fastq.gz docker exec fastqc_container_test fastqc /scratch/B7_input_s_chr19.fastq.gz # All (using wildcard *) docker exec fastqc_container_test bash -c &#39;ls /scratch/*gz&#39; bash -c stands for executing from the provided string. HANDS-ON Copy the 2 FASTA files from the Github repository and place them in mounted directory. Run blastp inside the container against each other: blastp -query /scratch/O75976.fasta -subject /scratch/Q90240.fasta Run blastp outside the container and get the result. Do with exec and do all at once with run Answer # Let&#39;s create the container and we dettach it docker run --detach -ti --volume $(pwd)/datatest:/scratch --name blastp_test ncbi/blast:2.10.1 # Execute container interactively and run blastp docker exec -ti blastp_test /bin/bash &gt; blastp -query /scratch/O75976.fasta -subject /scratch/Q90240.fasta &gt; /scratch/insideout.txt # Run from outside and retrieve in different ways docker exec -ti blastp_test blastp -query /scratch/O75976.fasta -subject /scratch/Q90240.fasta docker exec -ti blastp_test blastp -query /scratch/O75976.fasta -subject /scratch/Q90240.fasta &gt; out.txt docker exec -ti blastp_test blastp -query /scratch/O75976.fasta -subject /scratch/Q90240.fasta -out /scratch/outagain.txt docker run --volume $(pwd)/datatest:/scratch --name blastp1 ncbi/blast:2.10.1 blastp -query /scratch/O75976.fasta -subject /scratch/Q90240.fasta &gt; out2.txt docker run --volume $(pwd)/datatest:/scratch --name blastp2 ncbi/blast:2.10.1 blastp -query /scratch/O75976.fasta -subject /scratch/Q90240.fasta -out /scratch/outagain2.txt "],["ports.html", "3.9 Ports", " 3.9 Ports The same as with volumes, but with ports, they are used to enable Internet services. Syntax: --publish/-p host:container In the example below we use NGINX, a popular web server that, by default, exposes the port 80. # We start the web server in the background docker run --detach --name webserver nginx # We test with curl if port 80 is available from our host machine curl localhost:80 # We then check inside the container docker exec webserver curl localhost:80 # We remove the container docker rm -f webserver # We start the web server in the background. We use **publish** to map the ports from the container to the host docker run --detach --name webserver --publish 80:80 nginx # We test if port 80 is available from our host machine curl localhost:80 # We remove the container docker rm -f webserver # We start the web server in the background. We use **publish** to map the ports from the container to the host. Now instead of mapping to 80 in the host, we map to another one (3838 for instance). docker run --detach --name webserver -p 3838:80 nginx # We test if port 80 is available from our host machine curl localhost:80 # We test if the different mapped port is available from our host machine curl localhost:3838 # We repeat the check, but now inside the container docker exec webserver curl localhost:80 docker exec webserver curl localhost:3838 # We remove the container docker rm -f webserver You can check the website also from your web browser at: http://mymachine-address-here/ (port 80 by default). Replace your mymachine-address-here for machine provided address or 127.0.0.1/localhost in case you were trying it from your own machine. "],["integrative-examples.html", "3.10 Integrative examples", " 3.10 Integrative examples 3.10.1 FASTQC Web Application We work in a dummy FASTQC Web service We place B7_input_s_chr19.fastq.gz file from available datasets in $HOME/myscratch mkdir -p $HOME/myscratch cp testdata/* $HOME/myscratch cd containers/docker/fastqc_www docker build -t fastqcwww -f Dockerfile ../../scripts/fastqc docker run -d -v $HOME/myscratch:/scratch -p 3838:8083 --name myfastqc fastqcwww Example query from the browser: http://mymachine-address-here:3838/?file=B7_input_s_chr19.fastq As commented in previous sections, the context is the place from where Docker will start the build process. Keep it in mind when defining the paths when using commands such as COPY in the recipe. Replace your mymachine-address-here for machine provided address or 127.0.0.1/localhost in case you were trying it from your own machine. 3.10.2 Shiny Application cd containers/docker/shiny docker build -t shinyapp -f Dockerfile ../../scripts docker run -d -v $(pwd)/../../scripts/shiny:/srv/shiny-server/myserver -p 3838:3838 --name myserver shinyapp Check the result from the browser http://mymachine-address-here:3838 In the CMD execution you see the host 0.0.0.0, this a normal approach to indicate association to any IP address from the machine you launch the program. In a very machine you can have many routings and networks associated (e.g., more than one Ethernet plug) "],["publish-images.html", "3.11 Publish images", " 3.11 Publish images 3.11.1 Push an image Figure 3.1: PUSH. Steve Snodgrass. CC-BY, Source: https://www.flickr.com/photos/stevensnodgrass/6117660537/ If you didn’t do it yet, you can create an account in Docker Hub. Generate of an access key in Docker Hub Let’s go back to our terminal and we use that access token. Username is your Docker Hub username. docker login (base) [ec2-user@ip-172-31-47-200 ~]$ docker login Login with your Docker ID to push and pull images from Docker Hub. If you don&#39;t have a Docker ID, head over to https://hub.docker.com to create one. Username: myuser Password: WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Let’s create a repository in Docker Hub We use one image we have in our system and we add a tag that matches our repository name. docker pull biocontainers/fastqc:v0.11.9_cv7 Status: Downloaded newer image for biocontainers/fastqc:v0.11.9_cv7 docker.io/biocontainers/fastqc:v0.11.9_cv7 (base) [ec2-user@ip-172-31-45-86 ~]$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE biocontainers/fastqc v0.11.9_cv7 e5e3008d2bd1 6 months ago 834MB (base) [ec2-user@ip-172-31-45-86 ~]$ docker tag biocontainers/fastqc:v0.11.9_cv7 toniher/fastqc-example:latest (base) [ec2-user@ip-172-31-45-86 ~]$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE biocontainers/fastqc v0.11.9_cv7 e5e3008d2bd1 6 months ago 834MB toniher/fastqc-example latest e5e3008d2bd1 6 months ago 834MB Then we push that new added tag and in shortly you will see in Docker Hub page. docker push toniher/fastqc-example:latest As already noted, default registry is Docker Hub. If using another one, it is necessary to prepend it as when pulling (e.g., docker push quay.io/toniher/myrepo) Docker Hub PRO users have the option to have automatic builds generated when there is a change in an associated Git repository (Github). Alternatively, you can use Github Actions. "],["exercise-3---docker-volumes-and-ports.html", "3.12 Exercise 3 - Docker volumes and ports", " 3.12 Exercise 3 - Docker volumes and ports Simple BLAST web application. Including mount/volume and port. Description below Answer Example Dockerfile FROM debian:stretch # File Author / Maintainer MAINTAINER Toni Hermoso Pulido &lt;toni.hermoso@crg.eu&gt; ARG BLAST_VERSION=2.10.1 RUN apt-get update; apt-get install -y curl; RUN cd /usr/local; curl --fail --silent --show-error --location --remote-name ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/${BLAST_VERSION}/ncbi-blast-${BLAST_VERSION}+-x64-linux.tar.gz RUN cd /usr/local; tar zxf ncbi-blast-${BLAST_VERSION}+-x64-linux.tar.gz; rm ncbi-blast-${BLAST_VERSION}+-x64-linux.tar.gz RUN cd /usr/local/bin; ln -s /usr/local/ncbi-blast-${BLAST_VERSION}+/bin/* . # Default location of BLAST databases VOLUME /blastdb ENV BLASTDB /blastdb # Now adding web stuff RUN apt-get install -y php-cli RUN mkdir /wwww WORKDIR /wwww COPY index.php . # Clean cache RUN apt-get clean RUN set -x; rm -rf /var/lib/apt/lists/* EXPOSE 8081 ENTRYPOINT [&quot;php&quot;, &quot;-S&quot;, &quot;0.0.0.0:8081&quot;] docker run -d -v $HOME/db:/blastdb -p 80:8081 --name myblast blastwww Example query from the browser: http://mymachine-address-here/?id=O75976 Push resulting image to Docker Hub "],["singularity.html", "Part 4 Singularity", " Part 4 Singularity "],["introduction-to-singularity.html", "4.1 Introduction to Singularity", " 4.1 Introduction to Singularity Focus: Reproducibility to scientific computing and the high-performance computing (HPC) world. Origin: Lawrence Berkeley National Laboratory. Later spin-off: Sylabs Version 1.0 -&gt; 2016 More information: https://en.wikipedia.org/wiki/Singularity_(software) 4.1.1 Singularity architecture Strengths Weaknesses No dependency of a daemon At the time of writing only good support in LinuxMac experimental. Desktop edition. Only running Can be run as a simple userAvoids permission headaches and hacks For some features you need root account (or sudo) Image/container is a file (or directory) More easily portable Two types of images:Read-only (production)Writable (development, via sandbox) 4.1.2 Strengths No dependency of a daemon Can be run as a simple user Avoid permission headaches and hacks Image/container is a file (or directory) More easily portable Two type of images Read-only (production) Writable (development, via sandbox) 4.1.3 Weaknesses At the time of writing only good support in Linux Mac experimental. Desktop edition. Only running For some features you need root account (or sudo) "],["warning.html", "4.2 Warning", " 4.2 Warning There may be some confusion since there are two projects which the share the same name: HPCng Singularity Sylabs Singularity They “forked” not long ago. So far they share most of the codebase, but eventually this may different and software could have different functionality. "],["build-process.html", "4.3 Build process", " 4.3 Build process 4.3.1 Examples There are different ways to generate Singularity image files. The most common way is so far thanks to existing Docker images already present in public registries. 4.3.1.1 Through registries 4.3.1.1.1 Docker Hub https://hub.docker.com/r/biocontainers/fastqc singularity build fastqc-0.11.9_cv7.sif docker://biocontainers/fastqc:v0.11.9_cv7 4.3.1.1.2 Biocontainers 4.3.1.1.2.1 Via quay.io https://quay.io/repository/biocontainers/fastqc singularity build fastqc-0.11.9.sif docker://quay.io/biocontainers/fastqc:0.11.9--0 4.3.1.1.2.2 Via Galaxy project prebuilt images singularity pull --name fastqc-0.11.9.sif https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0 Galaxy project provides all Bioinformatics software from Biocontainers initiative as Singularity prebuilt images. If download and conversion time of images is an issue for you, this is likely the best option if working in the biomedical field. 4.3.1.1.3 Singularity catalog Not as popular so far, but it provides a lot of recipes (to be discussed below) https://singularityhub.github.io/singularity-catalog/ 4.3.1.2 Docker Daemon If you create your own images (as we did during the course) and you don’t want to share them publicly (and you have not access to any private image registry, either), you can convert your locally built Docker images into Singularity image files. singularity build fastqc-web-0.11.9.sif docker-daemon://fastqcwww 4.3.2 Sandboxing Instead of generating an image file, it is actually possible to use a whole directory with its contents. This is handy when specific changes may be needed. singularity build --sandbox ./sandbox docker://ubuntu:18.04 touch sandbox/etc/myetc.conf singularity build sandbox.sif ./sandbox Apart from testing or debugging, as we commented with Docker, we don’t recommend this approach because it makes reproducibility more difficult. 4.3.3 Singularity recipes Singularity provides its own build system and recipe syntax. Despite it is actually possible to generate images from scratch (known as bootstraping) thanks to these recipes, this is at time of writing far slower than converting from Docker ones. Docker has the advantage of saving every action line as a cached image. That is not happening with Singularity. When using recipes, it’s mandatory to have administrator permissions (e.g., as beeing root or via sudo). In any case, it can still be useful to boostrap an image derived from a previously existing one. Below we provide two common approaches: 4.3.3.1 Docker bootstrap Instead of converting a Docker image into a Singularity one, it’s possible to use one as a base one and modify it by using Singularity recipe syntax. BootStrap: docker From: biocontainers/fastqc:v0.11.9_cv7 %runscript echo &quot;Welcome to FastQC Image&quot; fastqc --version %post echo &quot;Image built&quot; sudo singularity build fastqc.sif docker.singularity The command %runscript would be equivalent to ENTRYPOINT/CMD in Docker. It is only triggered when using singularity run. This is useful if you want to hide from the user the complexity of a command-line path or an included custom script. 4.3.3.2 Debian bootstrap Alternatively, we can build the whole image from Debian/Ubuntu distribution. This will normally take a while. BootStrap: debootstrap OSVersion: bionic MirrorURL: http://fr.archive.ubuntu.com/ubuntu/ Include: build-essential curl python python-dev openjdk-11-jdk bzip2 zip unzip %runscript echo &quot;Welcome to my Singularity Image&quot; fastqc --version multiqc --version bowtie --version %post FASTQC_VERSION=0.11.9 MULTIQC_VERSION=1.9 BOWTIE_VERSION=1.3.0 cd /usr/local; curl -k -L https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v${FASTQC_VERSION}.zip &gt; fastqc.zip cd /usr/local; unzip fastqc.zip; rm fastqc.zip; chmod 775 FastQC/fastqc; ln -s /usr/local/FastQC/fastqc /usr/local/bin/fastqc cd /usr/local; curl --fail --silent --show-error --location --remote-name https://github.com/BenLangmead/bowtie/releases/download/v$BOWTIE_VERSION/bowtie-${BOWTIE_VERSION}-linux-x86_64.zip cd /usr/local; unzip -d /usr/local bowtie-${BOWTIE_VERSION}-linux-x86_64.zip cd /usr/local; rm bowtie-${BOWTIE_VERSION}-linux-x86_64.zip cd /usr/local/bin; ln -s ../bowtie-${BOWTIE_VERSION}-linux-x86_64/bowtie* . curl --fail --silent --show-error --location --remote-name https://bootstrap.pypa.io/pip/2.7/get-pip.py python get-pip.py pip install numpy matplotlib pip install -I multiqc==${MULTIQC_VERSION} echo &quot;Biocore image built&quot; %labels Maintainer Biocorecrg Version 0.1.0 sudo singularity build fastqc-multi-bowtie.sif debootstrap.singularity The available deboostrap recipes depend on the version you have installed in the system. Check at: /usr/share/debootstrap/scripts/ for more details. For Debian, instead of Ubuntu, you can replace mirrorURL with https://ftp.debian.org/debian/ More details at: https://singularity.hpcng.org/user-docs/3.7/appendix.html#build-debootstrap It’s possible to sign cryptographically your images, so third parties can verify they are coming from their actual authors. This implies some matters beyond the scope of this course, but you have some details if interested at: https://singularity.hpcng.org/user-docs/3.7/signNverify.html "],["run-and-execution-process.html", "4.4 Run and execution process", " 4.4 Run and execution process Once we have some image files (or directories) ready, we can run or favourite processes. 4.4.1 Singularity shell The straight-forward exploratory approach, equivalent to docker run -ti myimage /bin/shell. But with a more handy syntax. singularity shell fastqc-multi-bowtie.sif Move around the directories and notice the different isolation approach compared to Docker. You can access most of the host filesystem. 4.4.2 Singularity exec That is the most common way to execute Singularity (equivalent to docker exec). That would be the normal approach in HPC environments. singularity exec fastqc-multi-bowtie.sif fastqc 4.4.3 Singularity run This executes runscript from recipe definition (equivalent to docker run). Not so common for HPC uses. More for instances (servers). singularity run fastqc-multi-bowtie.sif 4.4.4 Environment control By default Singularity inherits our profile environment (e.g., PATH environment variable). This may be convenient for some circumstances, but it can also lead to unexpected problems if you are not aware, when your own environment clashes with the default one from the image. singularity shell -e fastqc-multi-bowtie.sif singularity exec -e fastqc-multi-bowtie.sif fastqc singularity run -e fastqc-multi-bowtie.sif Compare env command with and without -e modifier. singularity exec fastqc-multi-bowtie.sif env singularity exec -e fastqc-multi-bowtie.sif env 4.4.5 Execute from sandboxed images / directories singularity exec ./sandbox ls -l /etc/myetc.conf # We can see file created in the directory before singularity exec ./sandbox bash -c &#39;apt-get update &amp;&amp; apt-get install python&#39; # We cannot install python singularity exec --writable ./sandbox bash -c &#39;apt-get update &amp;&amp; apt-get install python&#39; # We needed to add writable parameter 4.4.6 Execute straight from a registry Image is actually downloaded (and if a Docker one, converted) and stored in Singularity cache directory. singularity exec docker://ncbi/blast:2.10.1 blastp -version "],["bind-paths-aka-volumes.html", "4.5 Bind paths (aka volumes)", " 4.5 Bind paths (aka volumes) Paths of host system mounted in the container Default ones, no need to mount them explicitly (for 3.7.x): $HOME , /sys:/sys , /proc:/proc, /tmp:/tmp, /var/tmp:/var/tmp, /etc/resolv.conf:/etc/resolv.conf, /etc/passwd:/etc/passwd, and $PWD https://singularity.hpcng.org/user-docs/3.7/bind_paths_and_mounts.html For others, need to be done explicitly (syntax: host:container) mkdir testdir touch testdir/testout singularity shell -e -B ./testdir:/scratch fastqc-multi-bowtie.sif &gt; touch /scratch/testin &gt; exit ls -l testdir "],["example-execution.html", "4.6 Example execution:", " 4.6 Example execution: Using the 2 fastqc available files, process them outside and inside the mounted directory. Let’s use our fastqc.sif image file and we place it in $HOME # Let&#39;s create a dummy directory mkdir $HOME/scratch # Let&#39;s copy contents of testdata in scratch singularity exec fastqc.sif fastqc scratch/*fastq.gz # Check you have some HTMLs there. Remove them rm scratch/*html # Let&#39;s use shell singularity shell fastqc.sif &gt; cd scratch &gt; fastqc *fastq.gz &gt; exit # Check you have some HTMLs there. Remove them singularity exec -B ./scratch:/fastqcdir fastqc.sif fastqc /fastqcdir/*fastq.gz # What happens here! singularity exec -B ./scratch:/fastqcdir fastqc.sif bash -c &#39;fastqc /fastqcdir/*fastq.gz&#39; "],["instances.html", "4.7 Instances", " 4.7 Instances Also know as services. Despite Docker it is still more convenient for these tasks, it allows enabling thing such as webservices (e.g., via APIs) in HPC workflows. Instead of defining a %runscript block, we define a %startscript one. So, it would be possible to have both if needed. Simple example: Bootstrap: docker From: library/mariadb:10.3 %startscript mysqld sudo singularity build mariadb.sif mariadb.singularity mkdir -p testdir mkdir -p testdir/db mkdir -p testdir/socket singularity exec -B ./testdir/db:/var/lib/mysql mariadb.sif mysql_install_db singularity instance start -B ./testdir/db:/var/lib/mysql -B ./testdir/socket:/run/mysqld mariadb.sif mydb singularity instance list singularity exec instance://mydb mysql -uroot singularity instance stop mydb When exposing ports below 1024 in a LINUX machine, administrator (sudo) privileges are needed. Historical reference: https://www.w3.org/Daemon/User/Installation/PrivilegedPorts.html Specially with instances (but not only), if you encounter some permission problems you may need to enable write permissions in the image. If you do not want changes to persist in the image, you may want to use the –writable-tmpfs option. Changes are stored in an in-memory temporary filesystem which is discarded as soon as the service stops. More details about special storage options with Singularity: https://singularity.hpcng.org/user-docs/3.7/persistent_overlays.html More information: https://singularity.hpcng.org/user-docs/3.7/running_services.html https://singularity.hpcng.org/user-docs/3.7/networking.html "],["troubleshooting.html", "4.8 Troubleshooting", " 4.8 Troubleshooting singularity --help 4.8.1 Fakeroot Singularity permissions are an evolving field. If you don’t have access to sudo, it might be worth considering using --fakeroot/-f parameter. More details at https://singularity.hpcng.org/user-docs/3.7/fakeroot.html 4.8.2 Singularity cache directory $HOME/.singularity It stores cached images from registries, instances, etc. If problems may be a good place to clean. When running sudo, $HOME is /root. 4.8.3 Global singularity configuration Normally at /etc/singularity/singularity.conf or similar (e.g preceded by /usr/local/ is Singularity is installed manually) It can only be modified by users with administration permissions Worth noting bind path lines, which point default mounted directories in containers as commented in bind paths section "],["exercise-4---singularity-running-and-building.html", "4.9 Exercise 4 - Singularity running and building", " 4.9 Exercise 4 - Singularity running and building Example running BLAST commands in different ways Answer Compare with the previous Docker examples First of all, let’s generate a blast.sif image. We have plenty of ways to do this. One example below: singularity build blast.sif docker://ncbi/blast:2.10.1 4.9.1 Blast command-line (1) # If not there create a DB dir mkdir $HOME/db cp blast.sif $HOME/db cd $HOME/db curl -L https://www.uniprot.org/uniprot/O75976.fasta -o O75976.fasta curl -L https://www.uniprot.org/uniprot/Q90240.fasta -o Q90240.fasta singularity exec blast.sif blastp -query O75976.fasta -subject Q90240.fasta # We can mount if we prefer (as we did with Docker), but it&#39;s not strictly necessary singularity exec -B /home/ec2-user/db:/blastdb blast.sif blastp -query /blastdb/O75976.fasta -subject /blastdb/Q90240.fasta &gt; out.blast singularity exec -B /home/ec2-user/db:/blastdb blast.sif blastp -query /blastdb/O75976.fasta -subject /blastdb/Q90240.fasta -out /blastdb/output.blast 4.9.2 Blast command-line (2) # If not there create a DB dir mkdir $HOME/db cp blast.sif $HOME/db cd $HOME/db # Let&#39;s download Swissprot DB curl -L https://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/swissprot.gz -o swissprot.gz gunzip swissprot.gz # Let format the Swissprot DB singularity exec blast.sif makeblastdb -dbtype prot -parse_seqids -in swissprot We can retrieve a FASTA sequence by ID singularity exec blast.sif blastdbcmd -dbtype prot -db swissprot -entry O75976 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
